#!/usr/bin/env python3
"""
é˜¶æ®µ4: æ•°æ®é›†å‡†å¤‡

å‡†å¤‡å’Œé¢„å¤„ç†ChnSentiCorpæ•°æ®é›†ï¼ŒåŒ…æ‹¬æ•°æ®æ¸…æ´—ã€æ ‡æ³¨ã€åˆ†å‰²å’ŒéªŒè¯
"""

import os
import sys
import json
import time
import random
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_sample_dataset():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
    print("ğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®é›†...")
    
    # ç¤ºä¾‹ä¸­æ–‡æƒ…æ„Ÿæ•°æ®
    sample_data = [
        {"text": "è¿™ä¸ªäº§å“çœŸçš„å¾ˆæ£’ï¼Œè´¨é‡å¾ˆå¥½ï¼Œæ¨èè´­ä¹°ï¼", "label": "positive"},
        {"text": "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œå®Œå…¨ä¸æ»¡æ„ï¼Œä¸ä¼šå†æ¥äº†ã€‚", "label": "negative"},
        {"text": "ä»·æ ¼åˆç†ï¼Œæ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—æ¨èã€‚", "label": "positive"},
        {"text": "è´¨é‡ä¸€èˆ¬ï¼Œæ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¥½ã€‚", "label": "negative"},
        {"text": "ç‰©æµå¾ˆå¿«ï¼ŒåŒ…è£…ä¹Ÿå¾ˆå¥½ï¼Œå¾ˆæ»¡æ„ã€‚", "label": "positive"},
        {"text": "å®¢æœæ€åº¦ä¸å¥½ï¼Œè§£å†³é—®é¢˜å¾ˆæ…¢ã€‚", "label": "negative"},
        {"text": "åŠŸèƒ½å¾ˆå¼ºå¤§ï¼Œä½¿ç”¨èµ·æ¥å¾ˆæ–¹ä¾¿ã€‚", "label": "positive"},
        {"text": "ç•Œé¢è®¾è®¡ä¸å¤Ÿç¾è§‚ï¼Œç”¨æˆ·ä½“éªŒä¸€èˆ¬ã€‚", "label": "negative"},
        {"text": "æ€§ä»·æ¯”å¾ˆé«˜ï¼Œç‰©è¶…æ‰€å€¼ï¼Œå¼ºçƒˆæ¨èï¼", "label": "positive"},
        {"text": "å”®åæœåŠ¡å¾ˆå·®ï¼Œé—®é¢˜ä¸€ç›´æ²¡è§£å†³ã€‚", "label": "negative"}
    ]
    
    # æ‰©å±•æ•°æ®é›†
    extended_data = []
    for i in range(140):  # æ‰©å±•åˆ°1400ä¸ªæ ·æœ¬
        base_sample = sample_data[i % len(sample_data)]
        
        # æ·»åŠ å˜åŒ–
        variations = [
            "è¿™ä¸ª", "é‚£ä¸ª", "è¿™ä¸ª", "è¿™ä¸ª", "è¿™ä¸ª",
            "çœŸçš„", "ç¡®å®", "éå¸¸", "ç‰¹åˆ«", "ç›¸å½“",
            "å¾ˆ", "éå¸¸", "ç‰¹åˆ«", "ç›¸å½“", "ååˆ†",
            "æ£’", "å¥½", "ä¼˜ç§€", "å‡ºè‰²", "å“è¶Š",
            "å·®", "ç³Ÿç³•", "ä¸å¥½", "å·®åŠ²", "ç³Ÿç³•"
        ]
        
        text = base_sample["text"]
        label = base_sample["label"]
        
        # éšæœºæ·»åŠ ä¸€äº›å˜åŒ–
        if random.random() < 0.3:
            text = text.replace("å¾ˆ", random.choice(["éå¸¸", "ç‰¹åˆ«", "ç›¸å½“", "ååˆ†"]))
        if random.random() < 0.2:
            text = text.replace("è¿™ä¸ª", random.choice(["é‚£ä¸ª", "è¿™ä¸ª", "è¿™ä¸ª"]))
        
        extended_data.append({
            "text": text,
            "label": label
        })
    
    print(f"åˆ›å»ºäº† {len(extended_data)} ä¸ªç¤ºä¾‹æ•°æ®")
    return extended_data

def generate_pii_data(text: str, label: str) -> Dict[str, Any]:
    """ä¸ºæ–‡æœ¬ç”ŸæˆPIIæ•°æ®"""
    
    # æ¨¡æ‹ŸPIIå®ä½“
    pii_entities = []
    privacy_level = "low"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¯èƒ½çš„PII
    if any(keyword in text for keyword in ["ç”µè¯", "æ‰‹æœº", "è”ç³»"]):
        pii_entities.append({
            "type": "phone",
            "value": "138****8888",
            "confidence": 0.8
        })
        privacy_level = "medium"
    
    if any(keyword in text for keyword in ["é‚®ç®±", "é‚®ä»¶", "email"]):
        pii_entities.append({
            "type": "email", 
            "value": "user@example.com",
            "confidence": 0.9
        })
        privacy_level = "high"
    
    if any(keyword in text for keyword in ["å§“å", "åå­—", "ç§°å‘¼"]):
        pii_entities.append({
            "type": "name",
            "value": "å¼ **",
            "confidence": 0.7
        })
        privacy_level = "high"
    
    # ç”ŸæˆåˆæˆPIIæ•°æ®
    synthetic_pii = {
        "has_pii": len(pii_entities) > 0,
        "pii_count": len(pii_entities),
        "risk_score": len(pii_entities) * 0.3
    }
    
    # éšç§é¢„ç®—æˆæœ¬
    privacy_budget_cost = len(pii_entities) * 0.1 + random.uniform(0.01, 0.05)
    
    # è·¨å¢ƒä¼ è¾“æ£€æŸ¥
    pipl_cross_border = len(pii_entities) > 0 and random.random() < 0.1
    
    return {
        "pii_entities": pii_entities,
        "privacy_level": privacy_level,
        "synthetic_pii": synthetic_pii,
        "privacy_budget_cost": privacy_budget_cost,
        "pipl_cross_border": pipl_cross_border
    }

def process_dataset(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å¤„ç†æ•°æ®é›†ï¼Œæ·»åŠ éšç§ä¿æŠ¤ç›¸å…³å­—æ®µ"""
    print("ğŸ”’ å¤„ç†æ•°æ®é›†ï¼Œæ·»åŠ éšç§ä¿æŠ¤å­—æ®µ...")
    
    processed_data = []
    
    for i, item in enumerate(data):
        # ç”ŸæˆPIIæ•°æ®
        pii_data = generate_pii_data(item["text"], item["label"])
        
        # åˆ›å»ºå®Œæ•´çš„æ ·æœ¬
        sample = {
            "sample_id": f"sample_{i+1:04d}",
            "text": item["text"],
            "label": item["label"],
            "privacy_level": pii_data["privacy_level"],
            "pii_entities": pii_data["pii_entities"],
            "pipl_cross_border": pii_data["pipl_cross_border"],
            "synthetic_pii": pii_data["synthetic_pii"],
            "privacy_budget_cost": pii_data["privacy_budget_cost"],
            "metadata": {
                "text_length": len(item["text"]),
                "word_count": len(item["text"].split()),
                "created_at": datetime.now().isoformat(),
                "processing_stage": "preprocessing"
            }
        }
        
        processed_data.append(sample)
    
    print(f"å¤„ç†å®Œæˆï¼Œå…± {len(processed_data)} ä¸ªæ ·æœ¬")
    return processed_data

def split_dataset(data: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """åˆ†å‰²æ•°æ®é›†"""
    print("ğŸ“Š åˆ†å‰²æ•°æ®é›†...")
    
    # éšæœºæ‰“ä¹±æ•°æ®
    random.shuffle(data)
    
    # è®¡ç®—åˆ†å‰²ç‚¹
    total_samples = len(data)
    train_size = int(total_samples * 0.7)  # 70%
    val_size = int(total_samples * 0.15)  # 15%
    test_size = total_samples - train_size - val_size  # 15%
    
    # åˆ†å‰²æ•°æ®
    train_data = data[:train_size]
    val_data = data[train_size:train_size + val_size]
    test_data = data[train_size + val_size:]
    
    splits = {
        "train": train_data,
        "val": val_data,
        "test": test_data
    }
    
    print(f"æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_data)} ä¸ªæ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_data)} ä¸ªæ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_data)} ä¸ªæ ·æœ¬")
    
    return splits

def save_dataset_splits(splits: Dict[str, List[Dict[str, Any]]]):
    """ä¿å­˜æ•°æ®é›†åˆ†å‰²"""
    print("ğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ†å‰²...")
    
    data_dir = "/content/ianvs_pipl_framework/data/processed"
    os.makedirs(data_dir, exist_ok=True)
    
    for split_name, data in splits.items():
        file_path = os.path.join(data_dir, f"chnsenticorp_lite_{split_name}.jsonl")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            for sample in data:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        
        print(f"ä¿å­˜ {split_name} é›†: {file_path} ({len(data)} ä¸ªæ ·æœ¬)")
    
    return True

def convert_to_serializable(obj):
    """å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    else:
        return obj

def generate_dataset_statistics(splits: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
    """ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print("ğŸ“ˆ ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
    
    statistics = {
        "dataset_name": "ChnSentiCorp-Lite",
        "description": "ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼ˆè½»é‡ç‰ˆï¼‰",
        "created_at": datetime.now().isoformat(),
        "total_samples": sum(len(data) for data in splits.values()),
        "splits": {}
    }
    
    for split_name, data in splits.items():
        # åŸºæœ¬ç»Ÿè®¡
        total_samples = len(data)
        text_lengths = [len(sample["text"]) for sample in data]
        word_counts = [len(sample["text"].split()) for sample in data]
        
        # æ ‡ç­¾åˆ†å¸ƒ
        labels = [sample["label"] for sample in data]
        label_counts = pd.Series(labels).value_counts().to_dict()
        
        # éšç§çº§åˆ«åˆ†å¸ƒ
        privacy_levels = [sample["privacy_level"] for sample in data]
        privacy_counts = pd.Series(privacy_levels).value_counts().to_dict()
        
        # PIIç»Ÿè®¡
        pii_counts = [len(sample["pii_entities"]) for sample in data]
        has_pii_count = sum(1 for sample in data if len(sample["pii_entities"]) > 0)
        
        # è·¨å¢ƒä¼ è¾“ç»Ÿè®¡
        cross_border_count = sum(1 for sample in data if sample["pipl_cross_border"])
        
        # è®¡ç®—ç»Ÿè®¡å€¼å¹¶è½¬æ¢ä¸ºå¯åºåˆ—åŒ–ç±»å‹
        text_length_stats = {
            "mean": float(np.mean(text_lengths)),
            "std": float(np.std(text_lengths)),
            "min": int(np.min(text_lengths)),
            "max": int(np.max(text_lengths))
        }
        
        word_count_stats = {
            "mean": float(np.mean(word_counts)),
            "std": float(np.std(word_counts)),
            "min": int(np.min(word_counts)),
            "max": int(np.max(word_counts))
        }
        
        pii_stats = {
            "total_pii_entities": int(sum(pii_counts)),
            "samples_with_pii": int(has_pii_count),
            "pii_rate": float(has_pii_count / total_samples if total_samples > 0 else 0),
            "avg_pii_per_sample": float(np.mean(pii_counts))
        }
        
        cross_border_stats = {
            "cross_border_samples": int(cross_border_count),
            "cross_border_rate": float(cross_border_count / total_samples if total_samples > 0 else 0)
        }
        
        statistics["splits"][split_name] = {
            "samples": int(total_samples),
            "text_length": text_length_stats,
            "word_count": word_count_stats,
            "label_distribution": convert_to_serializable(label_counts),
            "privacy_level_distribution": convert_to_serializable(privacy_counts),
            "pii_statistics": pii_stats,
            "cross_border_statistics": cross_border_stats
        }
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_file = "/content/ianvs_pipl_framework/data/processed/statistics.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(convert_to_serializable(statistics), f, indent=2, ensure_ascii=False)
    
    print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")
    return statistics

def validate_dataset(splits: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
    """éªŒè¯æ•°æ®é›†"""
    print("âœ… éªŒè¯æ•°æ®é›†...")
    
    validation_results = {
        "timestamp": datetime.now().isoformat(),
        "validation_status": "passed",
        "issues": [],
        "recommendations": []
    }
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    for split_name, data in splits.items():
        if len(data) == 0:
            validation_results["issues"].append(f"{split_name} é›†ä¸ºç©º")
            validation_results["validation_status"] = "failed"
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ["sample_id", "text", "label", "privacy_level", "pii_entities"]
        for i, sample in enumerate(data):
            for field in required_fields:
                if field not in sample:
                    validation_results["issues"].append(f"{split_name} é›†æ ·æœ¬ {i} ç¼ºå°‘å­—æ®µ {field}")
                    validation_results["validation_status"] = "failed"
        
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        text_lengths = [len(sample["text"]) for sample in data]
        if max(text_lengths) > 1000:
            validation_results["recommendations"].append(f"{split_name} é›†åŒ…å«è¿‡é•¿çš„æ–‡æœ¬")
        
        if min(text_lengths) < 5:
            validation_results["recommendations"].append(f"{split_name} é›†åŒ…å«è¿‡çŸ­çš„æ–‡æœ¬")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    for split_name, data in splits.items():
        labels = [sample["label"] for sample in data]
        label_counts = pd.Series(labels).value_counts()
        
        if len(label_counts) < 2:
            validation_results["issues"].append(f"{split_name} é›†æ ‡ç­¾ç§ç±»ä¸è¶³")
            validation_results["validation_status"] = "failed"
        
        # æ£€æŸ¥æ ‡ç­¾å¹³è¡¡æ€§
        max_count = label_counts.max()
        min_count = label_counts.min()
        if max_count / min_count > 3:
            validation_results["recommendations"].append(f"{split_name} é›†æ ‡ç­¾åˆ†å¸ƒä¸å‡è¡¡")
    
    # ä¿å­˜éªŒè¯ç»“æœ
    validation_file = "/content/ianvs_pipl_framework/data/processed/validation_report.json"
    with open(validation_file, 'w', encoding='utf-8') as f:
        json.dump(validation_results, f, indent=2, ensure_ascii=False)
    
    print(f"éªŒè¯ç»“æœå·²ä¿å­˜: {validation_file}")
    print(f"éªŒè¯çŠ¶æ€: {validation_results['validation_status']}")
    
    if validation_results["issues"]:
        print("å‘ç°çš„é—®é¢˜:")
        for issue in validation_results["issues"]:
            print(f"  - {issue}")
    
    if validation_results["recommendations"]:
        print("å»ºè®®:")
        for rec in validation_results["recommendations"]:
            print(f"  - {rec}")
    
    return validation_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ é˜¶æ®µ4: æ•°æ®é›†å‡†å¤‡")
    print("=" * 50)
    
    try:
        # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
        sample_data = create_sample_dataset()
        
        # 2. å¤„ç†æ•°æ®é›†
        processed_data = process_dataset(sample_data)
        
        # 3. åˆ†å‰²æ•°æ®é›†
        splits = split_dataset(processed_data)
        
        # 4. ä¿å­˜æ•°æ®é›†åˆ†å‰²
        save_dataset_splits(splits)
        
        # 5. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = generate_dataset_statistics(splits)
        
        # 6. éªŒè¯æ•°æ®é›†
        validation_results = validate_dataset(splits)
        
        # 7. ä¿å­˜å‡†å¤‡æŠ¥å‘Š
        preparation_report = {
            "timestamp": datetime.now().isoformat(),
            "total_samples": len(processed_data),
            "splits": {
                "train": len(splits["train"]),
                "val": len(splits["val"]),
                "test": len(splits["test"])
            },
            "statistics_file": "/content/ianvs_pipl_framework/data/processed/statistics.json",
            "validation_file": "/content/ianvs_pipl_framework/data/processed/validation_report.json",
            "validation_status": validation_results["validation_status"],
            "issues_count": len(validation_results["issues"]),
            "recommendations_count": len(validation_results["recommendations"])
        }
        
        report_file = '/content/ianvs_pipl_framework/logs/dataset_preparation_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(preparation_report, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
        print(f"æ€»æ ·æœ¬æ•°: {len(processed_data)}")
        print(f"è®­ç»ƒé›†: {len(splits['train'])} ä¸ªæ ·æœ¬")
        print(f"éªŒè¯é›†: {len(splits['val'])} ä¸ªæ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(splits['test'])} ä¸ªæ ·æœ¬")
        print(f"ç»Ÿè®¡ä¿¡æ¯: /content/ianvs_pipl_framework/data/processed/statistics.json")
        print(f"éªŒè¯æŠ¥å‘Š: /content/ianvs_pipl_framework/data/processed/validation_report.json")
        print(f"å‡†å¤‡æŠ¥å‘Š: {report_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
        logger.error(f"æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ é˜¶æ®µ4å®Œæˆï¼Œå¯ä»¥ç»§ç»­æ‰§è¡Œé˜¶æ®µ5")
    else:
        print("\nâŒ é˜¶æ®µ4å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
