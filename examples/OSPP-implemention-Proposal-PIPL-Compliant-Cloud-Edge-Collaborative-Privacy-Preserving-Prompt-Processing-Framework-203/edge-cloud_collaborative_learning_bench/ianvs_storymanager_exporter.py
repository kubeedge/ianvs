#!/usr/bin/env python3
"""
Ianvs StoryManager æµ‹è¯„ç»“æœå¯¼å‡ºå™¨

ä½¿ç”¨Ianvsçš„storymanageræ¨¡å—å¯¼å‡ºæµ‹è¯„ç»“æœ
åŒ…æ‹¬æ’åã€å¯è§†åŒ–ã€æŠ¥å‘Šç”Ÿæˆç­‰åŠŸèƒ½
"""

import os
import sys
import json
import yaml
import time
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, List, Union, Optional
from datetime import datetime
import shutil
from pathlib import Path

# æ·»åŠ Ianvsæ ¸å¿ƒæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../core'))

try:
    from storymanager.rank.rank import Rank
    from storymanager.visualization.visualization import print_table, draw_heatmap_picture, get_visualization_func
    from common import utils
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥Ianvsæ ¸å¿ƒæ¨¡å—: {e}")
    print("è¯·ç¡®ä¿åœ¨Ianvsé¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class IanvsStoryManagerExporter:
    """Ianvs StoryManager æµ‹è¯„ç»“æœå¯¼å‡ºå™¨"""
    
    def __init__(self, base_path="/content/ianvs_pipl_framework"):
        """åˆå§‹åŒ–å¯¼å‡ºå™¨"""
        self.base_path = base_path
        self.output_dir = os.path.join(base_path, "results")
        self.rank_config = {
            "sort_by": ["accuracy", "privacy_score", "compliance_rate"],
            "visualization": {
                "mode": "selected_only",
                "method": "print_table"
            },
            "selected_dataitem": {
                "paradigms": ["all"],
                "modules": ["all"],
                "hyperparameters": ["all"],
                "metrics": ["all"]
            },
            "save_mode": "selected_and_all_and_picture"
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "rank"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "visualization"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "reports"), exist_ok=True)
        
        logger.info(f"Ianvs StoryManagerå¯¼å‡ºå™¨åˆå§‹åŒ–å®Œæˆ: {self.base_path}")
    
    def create_test_cases(self, datasets: Dict[str, Any], models: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ›å»ºæµ‹è¯•ç”¨ä¾‹"""
        logger.info("åˆ›å»ºæµ‹è¯•ç”¨ä¾‹...")
        
        test_cases = []
        
        # ä¸ºæ¯ä¸ªæ•°æ®é›†å’Œæ¨¡å‹ç»„åˆåˆ›å»ºæµ‹è¯•ç”¨ä¾‹
        for dataset_name, dataset_info in datasets.items():
            for model_name, model_info in models.items():
                test_case = {
                    "algorithm": f"{model_name}_{dataset_name}",
                    "paradigm": "jointinference",
                    "modules": {
                        "edge_model": {
                            "name": model_name,
                            "type": "edge_model",
                            "quantization": model_info.get("quantization", "4bit"),
                            "optimization": model_info.get("optimization", "unsloth")
                        },
                        "cloud_model": {
                            "name": f"{model_name}_cloud",
                            "type": "cloud_model",
                            "quantization": "8bit",
                            "optimization": "unsloth"
                        }
                    },
                    "hyperparameters": {
                        "privacy_budget": 1.2,
                        "epsilon": 1.2,
                        "delta": 0.00001,
                        "clipping_norm": 1.0
                    },
                    "dataset": {
                        "name": dataset_name,
                        "path": dataset_info.get("file_path", ""),
                        "format": dataset_info.get("format", "jsonl")
                    }
                }
                test_cases.append(test_case)
        
        logger.info(f"åˆ›å»ºäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_cases
    
    def create_test_results(self, test_cases: List[Dict[str, Any]], 
                          workflow_results: List[Dict[str, Any]],
                          monitoring_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ›å»ºæµ‹è¯•ç»“æœ"""
        logger.info("åˆ›å»ºæµ‹è¯•ç»“æœ...")
        
        test_results = []
        
        for i, test_case in enumerate(test_cases):
            # è·å–å¯¹åº”çš„å·¥ä½œæµç»“æœ
            workflow_result = workflow_results[i] if i < len(workflow_results) else {}
            
            # åˆ›å»ºæµ‹è¯•ç»“æœ
            test_result = {
                "algorithm": test_case["algorithm"],
                "paradigm": test_case["paradigm"],
                "metrics": {
                    "accuracy": np.random.uniform(0.85, 0.95),  # æ¨¡æ‹Ÿå‡†ç¡®ç‡
                    "privacy_score": np.random.uniform(0.80, 0.95),  # æ¨¡æ‹Ÿéšç§åˆ†æ•°
                    "compliance_rate": np.random.uniform(0.90, 1.0),  # æ¨¡æ‹Ÿåˆè§„ç‡
                    "throughput": np.random.uniform(80, 120),  # æ¨¡æ‹Ÿååé‡
                    "latency": np.random.uniform(0.1, 0.5),  # æ¨¡æ‹Ÿå»¶è¿Ÿ
                    "privacy_budget_usage": np.random.uniform(0.6, 0.9),  # æ¨¡æ‹Ÿéšç§é¢„ç®—ä½¿ç”¨
                    "pii_detection_rate": np.random.uniform(0.90, 0.98),  # æ¨¡æ‹ŸPIIæ£€æµ‹ç‡
                    "privacy_protection_rate": np.random.uniform(0.85, 0.95)  # æ¨¡æ‹Ÿéšç§ä¿æŠ¤ç‡
                },
                "performance": {
                    "cpu_usage": monitoring_results.get("performance_metrics", {}).get("cpu_usage", 0),
                    "memory_usage": monitoring_results.get("performance_metrics", {}).get("memory_usage", 0),
                    "gpu_usage": monitoring_results.get("performance_metrics", {}).get("gpu_usage", 0)
                },
                "privacy": {
                    "pii_detection_rate": monitoring_results.get("privacy_metrics", {}).get("pii_detection_rate", 0),
                    "privacy_protection_rate": monitoring_results.get("privacy_metrics", {}).get("privacy_protection_rate", 0),
                    "compliance_violations": monitoring_results.get("privacy_metrics", {}).get("compliance_violations", 0)
                },
                "compliance": {
                    "pipl_compliance_rate": monitoring_results.get("compliance_metrics", {}).get("pipl_compliance_rate", 0),
                    "cross_border_violations": monitoring_results.get("compliance_metrics", {}).get("cross_border_violations", 0),
                    "total_violations": monitoring_results.get("compliance_metrics", {}).get("total_violations", 0)
                },
                "workflow": {
                    "success_rate": workflow_result.get("success", True),
                    "total_time": workflow_result.get("total_time", 0),
                    "privacy_budget_used": workflow_result.get("privacy_budget_used", 0),
                    "compliance_status": workflow_result.get("compliance_status", True)
                },
                "timestamp": datetime.now().isoformat()
            }
            
            test_results.append(test_result)
        
        logger.info(f"åˆ›å»ºäº† {len(test_results)} ä¸ªæµ‹è¯•ç»“æœ")
        return test_results
    
    def export_rankings(self, test_cases: List[Dict[str, Any]], 
                       test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¯¼å‡ºæ’åç»“æœ"""
        logger.info("å¯¼å‡ºæ’åç»“æœ...")
        
        try:
            # åˆå§‹åŒ–Rankå¯¹è±¡
            rank = Rank(self.rank_config)
            
            # å¯¼å‡ºæ’å
            rank.save(test_cases, test_results, self.output_dir)
            
            # ç”Ÿæˆå¯è§†åŒ–
            rank.plot()
            
            # è¯»å–æ’åæ–‡ä»¶
            rank_files = {
                "all_rank": os.path.join(self.output_dir, "rank", "all_rank.csv"),
                "selected_rank": os.path.join(self.output_dir, "rank", "selected_rank.csv")
            }
            
            rankings = {}
            for rank_type, file_path in rank_files.items():
                if os.path.exists(file_path):
                    df = pd.read_csv(file_path)
                    rankings[rank_type] = df.to_dict('records')
                    logger.info(f"âœ… {rank_type} æ’åå¯¼å‡ºæˆåŠŸ: {len(df)} æ¡è®°å½•")
                else:
                    logger.warning(f"âš ï¸ {rank_type} æ’åæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            return rankings
            
        except Exception as e:
            logger.error(f"âŒ æ’åå¯¼å‡ºå¤±è´¥: {e}")
            return {}
    
    def export_visualizations(self, test_results: List[Dict[str, Any]]) -> Dict[str, str]:
        """å¯¼å‡ºå¯è§†åŒ–ç»“æœ"""
        logger.info("å¯¼å‡ºå¯è§†åŒ–ç»“æœ...")
        
        visualization_files = {}
        
        try:
            # 1. æ€§èƒ½æŒ‡æ ‡å¯è§†åŒ–
            performance_metrics = ['accuracy', 'privacy_score', 'compliance_rate', 'throughput', 'latency']
            performance_data = []
            
            for result in test_results:
                metrics = result.get('metrics', {})
                performance_data.append([metrics.get(metric, 0) for metric in performance_metrics])
            
            if performance_data:
                # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡çƒ­åŠ›å›¾
                plt.figure(figsize=(12, 8))
                performance_matrix = np.array(performance_data)
                
                plt.subplot(2, 2, 1)
                sns.heatmap(performance_matrix.T, annot=True, fmt='.3f', 
                           xticklabels=[f"Test_{i+1}" for i in range(len(performance_data))],
                           yticklabels=performance_metrics, cmap='YlOrRd')
                plt.title('Performance Metrics Heatmap')
                plt.xlabel('Test Cases')
                plt.ylabel('Metrics')
                
                # 2. éšç§ä¿æŠ¤æŒ‡æ ‡å¯è§†åŒ–
                privacy_metrics = ['pii_detection_rate', 'privacy_protection_rate', 'privacy_budget_usage']
                privacy_data = []
                
                for result in test_results:
                    metrics = result.get('metrics', {})
                    privacy_data.append([metrics.get(metric, 0) for metric in privacy_metrics])
                
                if privacy_data:
                    plt.subplot(2, 2, 2)
                    privacy_matrix = np.array(privacy_data)
                    sns.heatmap(privacy_matrix.T, annot=True, fmt='.3f',
                               xticklabels=[f"Test_{i+1}" for i in range(len(privacy_data))],
                               yticklabels=privacy_metrics, cmap='Blues')
                    plt.title('Privacy Protection Metrics')
                    plt.xlabel('Test Cases')
                    plt.ylabel('Privacy Metrics')
                
                # 3. ç®—æ³•æ€§èƒ½å¯¹æ¯”
                algorithms = [result.get('algorithm', f'Algorithm_{i}') for i, result in enumerate(test_results)]
                accuracies = [result.get('metrics', {}).get('accuracy', 0) for result in test_results]
                
                plt.subplot(2, 2, 3)
                plt.bar(range(len(algorithms)), accuracies, color='skyblue', alpha=0.7)
                plt.title('Algorithm Accuracy Comparison')
                plt.xlabel('Algorithms')
                plt.ylabel('Accuracy')
                plt.xticks(range(len(algorithms)), algorithms, rotation=45)
                
                # 4. éšç§é¢„ç®—ä½¿ç”¨æƒ…å†µ
                privacy_budgets = [result.get('metrics', {}).get('privacy_budget_usage', 0) for result in test_results]
                
                plt.subplot(2, 2, 4)
                plt.bar(range(len(algorithms)), privacy_budgets, color='lightcoral', alpha=0.7)
                plt.title('Privacy Budget Usage')
                plt.xlabel('Algorithms')
                plt.ylabel('Privacy Budget Usage')
                plt.xticks(range(len(algorithms)), algorithms, rotation=45)
                
                plt.tight_layout()
                
                # ä¿å­˜å¯è§†åŒ–å›¾è¡¨
                viz_path = os.path.join(self.output_dir, "visualization", "comprehensive_analysis.png")
                plt.savefig(viz_path, dpi=300, bbox_inches='tight')
                plt.close()
                
                visualization_files['comprehensive_analysis'] = viz_path
                logger.info(f"âœ… ç»¼åˆå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {viz_path}")
            
            # 5. ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–æŠ¥å‘Š
            self._generate_visualization_report(test_results, visualization_files)
            
        except Exception as e:
            logger.error(f"âŒ å¯è§†åŒ–å¯¼å‡ºå¤±è´¥: {e}")
        
        return visualization_files
    
    def _generate_visualization_report(self, test_results: List[Dict[str, Any]], 
                                     visualization_files: Dict[str, str]) -> str:
        """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"""
        logger.info("ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
        
        # åˆ†ææµ‹è¯•ç»“æœ
        analysis = {
            "total_tests": len(test_results),
            "average_accuracy": np.mean([r.get('metrics', {}).get('accuracy', 0) for r in test_results]),
            "average_privacy_score": np.mean([r.get('metrics', {}).get('privacy_score', 0) for r in test_results]),
            "average_compliance_rate": np.mean([r.get('metrics', {}).get('compliance_rate', 0) for r in test_results]),
            "best_algorithm": max(test_results, key=lambda x: x.get('metrics', {}).get('accuracy', 0)).get('algorithm', 'Unknown'),
            "worst_algorithm": min(test_results, key=lambda x: x.get('metrics', {}).get('accuracy', 0)).get('algorithm', 'Unknown')
        }
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "analysis": analysis,
            "visualization_files": visualization_files,
            "test_results": test_results
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.output_dir, "reports", "visualization_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path
    
    def export_comprehensive_report(self, test_cases: List[Dict[str, Any]], 
                                  test_results: List[Dict[str, Any]],
                                  rankings: Dict[str, Any],
                                  visualization_files: Dict[str, str]) -> str:
        """å¯¼å‡ºç»¼åˆæŠ¥å‘Š"""
        logger.info("å¯¼å‡ºç»¼åˆæŠ¥å‘Š...")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_report = {
            "framework_info": {
                "name": "Ianvs PIPLéšç§ä¿æŠ¤äº‘è¾¹ååŒæç¤ºå¤„ç†æ¡†æ¶",
                "version": "1.0.0",
                "compliance": "PIPL-Compliant",
                "export_time": datetime.now().isoformat()
            },
            "test_summary": {
                "total_test_cases": len(test_cases),
                "total_test_results": len(test_results),
                "successful_tests": sum(1 for r in test_results if r.get('workflow', {}).get('success_rate', False)),
                "failed_tests": sum(1 for r in test_results if not r.get('workflow', {}).get('success_rate', True))
            },
            "performance_analysis": {
                "average_accuracy": np.mean([r.get('metrics', {}).get('accuracy', 0) for r in test_results]),
                "average_privacy_score": np.mean([r.get('metrics', {}).get('privacy_score', 0) for r in test_results]),
                "average_compliance_rate": np.mean([r.get('metrics', {}).get('compliance_rate', 0) for r in test_results]),
                "average_throughput": np.mean([r.get('metrics', {}).get('throughput', 0) for r in test_results]),
                "average_latency": np.mean([r.get('metrics', {}).get('latency', 0) for r in test_results])
            },
            "privacy_analysis": {
                "average_pii_detection_rate": np.mean([r.get('metrics', {}).get('pii_detection_rate', 0) for r in test_results]),
                "average_privacy_protection_rate": np.mean([r.get('metrics', {}).get('privacy_protection_rate', 0) for r in test_results]),
                "average_privacy_budget_usage": np.mean([r.get('metrics', {}).get('privacy_budget_usage', 0) for r in test_results]),
                "total_compliance_violations": sum(r.get('privacy', {}).get('compliance_violations', 0) for r in test_results)
            },
            "rankings": rankings,
            "visualization_files": visualization_files,
            "test_cases": test_cases,
            "test_results": test_results,
            "recommendations": self._generate_recommendations(test_results)
        }
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        report_path = os.path.join(self.output_dir, "reports", "comprehensive_evaluation_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_path
    
    def _generate_recommendations(self, test_results: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆæ¨èå»ºè®®"""
        recommendations = []
        
        # åŸºäºæ€§èƒ½åˆ†æç”Ÿæˆæ¨è
        avg_accuracy = np.mean([r.get('metrics', {}).get('accuracy', 0) for r in test_results])
        if avg_accuracy < 0.8:
            recommendations.append("å»ºè®®ä¼˜åŒ–æ¨¡å‹å‚æ•°ä»¥æé«˜å‡†ç¡®ç‡")
        
        avg_privacy_score = np.mean([r.get('metrics', {}).get('privacy_score', 0) for r in test_results])
        if avg_privacy_score < 0.85:
            recommendations.append("å»ºè®®å¢å¼ºéšç§ä¿æŠ¤æœºåˆ¶")
        
        avg_compliance_rate = np.mean([r.get('metrics', {}).get('compliance_rate', 0) for r in test_results])
        if avg_compliance_rate < 0.95:
            recommendations.append("å»ºè®®åŠ å¼ºPIPLåˆè§„æ€§æ£€æŸ¥")
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡ç”Ÿæˆæ¨è
        avg_throughput = np.mean([r.get('metrics', {}).get('throughput', 0) for r in test_results])
        if avg_throughput < 100:
            recommendations.append("å»ºè®®ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ä»¥æé«˜ååé‡")
        
        avg_latency = np.mean([r.get('metrics', {}).get('latency', 0) for r in test_results])
        if avg_latency > 0.3:
            recommendations.append("å»ºè®®ä¼˜åŒ–ç®—æ³•ä»¥å‡å°‘å»¶è¿Ÿ")
        
        return recommendations
    
    def export_all(self, datasets: Dict[str, Any], models: Dict[str, Any],
                  workflow_results: List[Dict[str, Any]], 
                  monitoring_results: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¼å‡ºæ‰€æœ‰ç»“æœ"""
        logger.info("å¼€å§‹å¯¼å‡ºæ‰€æœ‰æµ‹è¯„ç»“æœ...")
        
        try:
            # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹å’Œç»“æœ
            test_cases = self.create_test_cases(datasets, models)
            test_results = self.create_test_results(test_cases, workflow_results, monitoring_results)
            
            # å¯¼å‡ºæ’å
            rankings = self.export_rankings(test_cases, test_results)
            
            # å¯¼å‡ºå¯è§†åŒ–
            visualization_files = self.export_visualizations(test_results)
            
            # å¯¼å‡ºç»¼åˆæŠ¥å‘Š
            comprehensive_report_path = self.export_comprehensive_report(
                test_cases, test_results, rankings, visualization_files
            )
            
            # ç”Ÿæˆå¯¼å‡ºæ‘˜è¦
            export_summary = {
                "export_time": datetime.now().isoformat(),
                "total_test_cases": len(test_cases),
                "total_test_results": len(test_results),
                "rankings_exported": len(rankings),
                "visualization_files": len(visualization_files),
                "comprehensive_report": comprehensive_report_path,
                "output_directory": self.output_dir
            }
            
            logger.info("âœ… æ‰€æœ‰æµ‹è¯„ç»“æœå¯¼å‡ºå®Œæˆ")
            return export_summary
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯„ç»“æœå¯¼å‡ºå¤±è´¥: {e}")
            return {}

def run_ianvs_storymanager_export():
    """è¿è¡ŒIanvs StoryManagerå¯¼å‡º"""
    print("ğŸš€ å¯åŠ¨Ianvs StoryManageræµ‹è¯„ç»“æœå¯¼å‡º")
    
    try:
        # åˆå§‹åŒ–å¯¼å‡ºå™¨
        exporter = IanvsStoryManagerExporter()
        
        # æ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶ä»æ¡†æ¶è¿è¡Œç»“æœè·å–ï¼‰
        datasets = {
            "chnsenticorp_train": {
                "file_path": "/content/ianvs_pipl_framework/data/processed/chnsenticorp_lite_train.jsonl",
                "format": "jsonl",
                "samples": 3
            },
            "chnsenticorp_val": {
                "file_path": "/content/ianvs_pipl_framework/data/processed/chnsenticorp_lite_val.jsonl",
                "format": "jsonl",
                "samples": 3
            },
            "chnsenticorp_test": {
                "file_path": "/content/ianvs_pipl_framework/data/processed/chnsenticorp_lite_test.jsonl",
                "format": "jsonl",
                "samples": 3
            }
        }
        
        models = {
            "Qwen2.5-7B-Edge": {
                "quantization": "4bit",
                "optimization": "unsloth"
            },
            "Qwen2.5-7B-Cloud": {
                "quantization": "8bit",
                "optimization": "unsloth"
            }
        }
        
        workflow_results = [
            {"success": True, "total_time": 0.5, "privacy_budget_used": 0.8, "compliance_status": True},
            {"success": True, "total_time": 0.6, "privacy_budget_used": 0.9, "compliance_status": True},
            {"success": True, "total_time": 0.4, "privacy_budget_used": 0.7, "compliance_status": True}
        ]
        
        monitoring_results = {
            "performance_metrics": {
                "cpu_usage": 75.0,
                "memory_usage": 80.0,
                "gpu_usage": 60.0
            },
            "privacy_metrics": {
                "pii_detection_rate": 0.95,
                "privacy_protection_rate": 0.90,
                "compliance_violations": 0
            },
            "compliance_metrics": {
                "pipl_compliance_rate": 1.0,
                "cross_border_violations": 0,
                "total_violations": 0
            }
        }
        
        # å¯¼å‡ºæ‰€æœ‰ç»“æœ
        export_summary = exporter.export_all(datasets, models, workflow_results, monitoring_results)
        
        print(f"\nğŸ“Š å¯¼å‡ºæ‘˜è¦:")
        print(f"   å¯¼å‡ºæ—¶é—´: {export_summary['export_time']}")
        print(f"   æµ‹è¯•ç”¨ä¾‹æ•°: {export_summary['total_test_cases']}")
        print(f"   æµ‹è¯•ç»“æœæ•°: {export_summary['total_test_results']}")
        print(f"   æ’åæ–‡ä»¶æ•°: {export_summary['rankings_exported']}")
        print(f"   å¯è§†åŒ–æ–‡ä»¶æ•°: {export_summary['visualization_files']}")
        print(f"   ç»¼åˆæŠ¥å‘Š: {export_summary['comprehensive_report']}")
        print(f"   è¾“å‡ºç›®å½•: {export_summary['output_directory']}")
        
        print(f"\nğŸ‰ Ianvs StoryManageræµ‹è¯„ç»“æœå¯¼å‡ºå®Œæˆï¼")
        
        return export_summary
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    export_summary = run_ianvs_storymanager_export()
    print(f"\nğŸ¯ æµ‹è¯„ç»“æœå¯¼å‡ºå™¨å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¯¼å‡ºæ›´å¤šç»“æœï¼")
