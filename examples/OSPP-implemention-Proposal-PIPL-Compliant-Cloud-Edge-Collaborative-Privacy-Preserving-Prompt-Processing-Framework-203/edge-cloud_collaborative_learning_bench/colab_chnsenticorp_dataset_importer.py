#!/usr/bin/env python3
"""
Colabç¯å¢ƒä¸‹çš„ChnSentiCorpæ•°æ®é›†å¯¼å…¥å’Œæµ‹è¯•è„šæœ¬

ä¸“é—¨é’ˆå¯¹æœ¬åœ°æ•°æ®é›†è·¯å¾„ï¼š
D:\ianvs\examples\OSPP-implemention-Proposal-PIPL-Compliant-Cloud-Edge-Collaborative-Privacy-Preserving-Prompt-Processing-Framework-203\edge-cloud_collaborative_learning_bench\data\chnsenticorp_lite

æ”¯æŒå¯¼å…¥ä»¥ä¸‹æ–‡ä»¶ï¼š
- train.jsonl (2000ä¸ªè®­ç»ƒæ ·æœ¬)
- test.jsonl (500ä¸ªæµ‹è¯•æ ·æœ¬) 
- val.jsonl (500ä¸ªéªŒè¯æ ·æœ¬)
- statistics.json (æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯)
- simple_validation_report.json (éªŒè¯æŠ¥å‘Š)
"""

import os
import sys
import json
import pandas as pd
import numpy as np
import time
import psutil
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, List, Union
import requests
from urllib.parse import urlparse
import zipfile
import tarfile
import shutil

def print_header(title: str):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ {title}")
    print(f"{'='*80}")

def print_step(step: str, status: str = "å¼€å§‹"):
    """æ‰“å°æ­¥éª¤"""
    print(f"\nğŸ“‹ {status}: {step}")

def print_success(message: str):
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print(f"âœ… {message}")

def print_warning(message: str):
    """æ‰“å°è­¦å‘Šä¿¡æ¯"""
    print(f"âš ï¸ {message}")

def print_error(message: str):
    """æ‰“å°é”™è¯¯ä¿¡æ¯"""
    print(f"âŒ {message}")

class ChnSentiCorpDatasetImporter:
    """ChnSentiCorpæ•°æ®é›†å¯¼å…¥å™¨"""
    
    def __init__(self, base_path: str = "/content/ianvs_pipl"):
        self.base_path = base_path
        self.datasets = {}
        self.statistics = {}
        self.validation_report = {}
        
        # è®¾ç½®å·¥ä½œç›®å½•
        os.makedirs(base_path, exist_ok=True)
        os.chdir(base_path)
        os.environ['PYTHONPATH'] = f'{base_path}/pipl_framework'
        
        print_success(f"ChnSentiCorpæ•°æ®é›†å¯¼å…¥å™¨åˆå§‹åŒ–å®Œæˆ: {base_path}")
    
    def upload_local_dataset(self, local_path: str, dataset_name: str = "chnsenticorp") -> bool:
        """ä¸Šä¼ æœ¬åœ°æ•°æ®é›†åˆ°Colab"""
        print_step(f"ä¸Šä¼ ChnSentiCorpæ•°æ®é›†: {local_path}")
        
        try:
            # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(local_path):
                print_error(f"æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {local_path}")
                return False
            
            # åœ¨Colabä¸­åˆ›å»ºç›®æ ‡ç›®å½•
            target_dir = os.path.join(self.base_path, dataset_name)
            os.makedirs(target_dir, exist_ok=True)
            
            # å¤åˆ¶æ•´ä¸ªç›®å½•
            print(f"ğŸ“ å¤åˆ¶ç›®å½•: {local_path} -> {target_dir}")
            shutil.copytree(local_path, target_dir, dirs_exist_ok=True)
            
            print_success(f"ChnSentiCorpæ•°æ®é›†ä¸Šä¼ å®Œæˆ: {dataset_name}")
            return True
            
        except Exception as e:
            print_error(f"æœ¬åœ°æ•°æ®é›†ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def import_chnsenticorp_dataset(self, colab_path: str) -> bool:
        """å¯¼å…¥ChnSentiCorpæ•°æ®é›†"""
        print_step("å¯¼å…¥ChnSentiCorpæ•°æ®é›†")
        
        try:
            # å¯¼å…¥è®­ç»ƒé›†
            train_file = os.path.join(colab_path, "train.jsonl")
            if os.path.exists(train_file):
                self.import_jsonl(train_file, "train")
            
            # å¯¼å…¥éªŒè¯é›†
            val_file = os.path.join(colab_path, "val.jsonl")
            if os.path.exists(val_file):
                self.import_jsonl(val_file, "val")
            
            # å¯¼å…¥æµ‹è¯•é›†
            test_file = os.path.join(colab_path, "test.jsonl")
            if os.path.exists(test_file):
                self.import_jsonl(test_file, "test")
            
            # å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯
            stats_file = os.path.join(colab_path, "statistics.json")
            if os.path.exists(stats_file):
                with open(stats_file, 'r', encoding='utf-8') as f:
                    self.statistics = json.load(f)
                print_success(f"ç»Ÿè®¡ä¿¡æ¯å¯¼å…¥å®Œæˆ: {stats_file}")
            
            # å¯¼å…¥éªŒè¯æŠ¥å‘Š
            report_file = os.path.join(colab_path, "simple_validation_report.json")
            if os.path.exists(report_file):
                with open(report_file, 'r', encoding='utf-8') as f:
                    self.validation_report = json.load(f)
                print_success(f"éªŒè¯æŠ¥å‘Šå¯¼å…¥å®Œæˆ: {report_file}")
            
            print_success("ChnSentiCorpæ•°æ®é›†å¯¼å…¥å®Œæˆ")
            return True
            
        except Exception as e:
            print_error(f"ChnSentiCorpæ•°æ®é›†å¯¼å…¥å¤±è´¥: {e}")
            return False
    
    def import_jsonl(self, file_path: str, dataset_name: str) -> bool:
        """å¯¼å…¥JSONLæ ¼å¼æ•°æ®é›†"""
        print_step(f"å¯¼å…¥JSONLæ•°æ®é›†: {dataset_name}")
        
        try:
            data = []
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        item = json.loads(line)
                        data.append(item)
                    except json.JSONDecodeError as e:
                        print_warning(f"ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue
            
            self.datasets[dataset_name] = {
                'data': data,
                'path': file_path,
                'format': 'jsonl',
                'size': len(data)
            }
            
            print_success(f"JSONLæ•°æ®é›†å¯¼å…¥å®Œæˆ: {len(data)} ä¸ªæ ·æœ¬")
            return True
            
        except Exception as e:
            print_error(f"JSONLå¯¼å…¥å¤±è´¥: {e}")
            return False
    
    def get_dataset_info(self, dataset_name: str) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        if dataset_name not in self.datasets:
            return {}
        
        dataset = self.datasets[dataset_name]
        data = dataset['data']
        
        # åˆ†ææ•°æ®é›†
        text_lengths = [len(item.get('text', '')) for item in data]
        labels = [item.get('label', 'unknown') for item in data]
        privacy_levels = [item.get('privacy_level', 'unknown') for item in data]
        pii_entities = [item.get('pii_entities', []) for item in data]
        
        # ç»Ÿè®¡PIIå®ä½“
        pii_count = sum(len(entities) for entities in pii_entities)
        
        info = {
            'name': dataset_name,
            'size': len(data),
            'format': dataset['format'],
            'path': dataset['path'],
            'text_stats': {
                'avg_length': np.mean(text_lengths),
                'min_length': np.min(text_lengths),
                'max_length': np.max(text_lengths),
                'std_length': np.std(text_lengths)
            },
            'label_distribution': dict(pd.Series(labels).value_counts()),
            'privacy_level_distribution': dict(pd.Series(privacy_levels).value_counts()),
            'pii_stats': {
                'total_pii_entities': pii_count,
                'avg_pii_per_sample': pii_count / len(data),
                'samples_with_pii': sum(1 for entities in pii_entities if len(entities) > 0)
            }
        }
        
        return info
    
    def list_datasets(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯¼å…¥çš„æ•°æ®é›†"""
        return list(self.datasets.keys())
    
    def get_dataset(self, dataset_name: str) -> List[Dict[str, Any]]:
        """è·å–æ•°æ®é›†æ•°æ®"""
        if dataset_name in self.datasets:
            return self.datasets[dataset_name]['data']
        return []
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        return self.statistics
    
    def get_validation_report(self) -> Dict[str, Any]:
        """è·å–éªŒè¯æŠ¥å‘Š"""
        return self.validation_report

class ChnSentiCorpDatasetTester:
    """ChnSentiCorpæ•°æ®é›†æµ‹è¯•å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results = {}
        self.performance_metrics = {}
        self.privacy_metrics = {}
        self.workflow_metrics = {}
    
    def test_dataset_performance(self, dataset_name: str, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®é›†æ€§èƒ½"""
        print_step(f"æµ‹è¯•æ•°æ®é›†æ€§èƒ½: {dataset_name}")
        
        start_time = time.time()
        start_memory = psutil.virtual_memory().percent
        
        # æ¨¡æ‹Ÿå¤„ç†
        processed_count = 0
        for item in data:
            # æ¨¡æ‹Ÿå¤„ç†
            time.sleep(0.001)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            processed_count += 1
        
        end_time = time.time()
        end_memory = psutil.virtual_memory().percent
        
        processing_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        performance_metrics = {
            'dataset_name': dataset_name,
            'sample_count': len(data),
            'processing_time': processing_time,
            'avg_time_per_sample': processing_time / len(data),
            'throughput': len(data) / processing_time,
            'memory_usage': memory_usage,
            'start_memory': start_memory,
            'end_memory': end_memory
        }
        
        self.performance_metrics[dataset_name] = performance_metrics
        
        print_success(f"æ€§èƒ½æµ‹è¯•å®Œæˆ:")
        print(f"   å¤„ç†æ ·æœ¬æ•°: {processed_count}")
        print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"   å¹³å‡æ—¶é—´: {processing_time/len(data):.4f}ç§’/æ ·æœ¬")
        print(f"   ååé‡: {len(data)/processing_time:.2f}æ ·æœ¬/ç§’")
        print(f"   å†…å­˜ä½¿ç”¨: {memory_usage:.2f}%")
        
        return performance_metrics
    
    def test_dataset_privacy(self, dataset_name: str, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®é›†éšç§ä¿æŠ¤"""
        print_step(f"æµ‹è¯•æ•°æ®é›†éšç§ä¿æŠ¤: {dataset_name}")
        
        privacy_results = []
        
        for item in data:
            text = item.get('text', '')
            pii_entities = item.get('pii_entities', [])
            privacy_level = item.get('privacy_level', 'general')
            pipl_cross_border = item.get('pipl_cross_border', True)
            
            # è®¡ç®—éšç§åˆ†æ•°
            privacy_score = len(pii_entities) / max(len(text), 1)
            
            privacy_results.append({
                'original_text': text,
                'pii_entities': pii_entities,
                'privacy_level': privacy_level,
                'pipl_cross_border': pipl_cross_border,
                'privacy_score': privacy_score
            })
        
        # è®¡ç®—éšç§ä¿æŠ¤æŒ‡æ ‡
        total_pii = sum(len(r['pii_entities']) for r in privacy_results)
        avg_privacy_score = np.mean([r['privacy_score'] for r in privacy_results])
        high_sensitivity_count = sum(1 for r in privacy_results if r['privacy_level'] == 'high_sensitivity')
        cross_border_count = sum(1 for r in privacy_results if r['pipl_cross_border'])
        
        privacy_metrics = {
            'dataset_name': dataset_name,
            'sample_count': len(data),
            'total_pii_detected': total_pii,
            'avg_privacy_score': avg_privacy_score,
            'privacy_protection_rate': 1.0 - avg_privacy_score,
            'high_sensitivity_ratio': high_sensitivity_count / len(data),
            'cross_border_ratio': cross_border_count / len(data)
        }
        
        self.privacy_metrics[dataset_name] = privacy_metrics
        
        print_success(f"éšç§ä¿æŠ¤æµ‹è¯•å®Œæˆ:")
        print(f"   æ£€æµ‹åˆ°PIIæ•°é‡: {total_pii}")
        print(f"   å¹³å‡éšç§åˆ†æ•°: {avg_privacy_score:.4f}")
        print(f"   éšç§ä¿æŠ¤ç‡: {1.0 - avg_privacy_score:.4f}")
        print(f"   é«˜æ•æ„Ÿåº¦æ¯”ä¾‹: {high_sensitivity_count/len(data):.4f}")
        print(f"   è·¨å¢ƒä¼ è¾“æ¯”ä¾‹: {cross_border_count/len(data):.4f}")
        
        return privacy_metrics
    
    def test_dataset_workflow(self, dataset_name: str, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®é›†ç«¯åˆ°ç«¯å·¥ä½œæµ"""
        print_step(f"æµ‹è¯•æ•°æ®é›†ç«¯åˆ°ç«¯å·¥ä½œæµ: {dataset_name}")
        
        workflow_results = []
        
        for item in data:
            text = item.get('text', '')
            label = item.get('label', 'unknown')
            privacy_level = item.get('privacy_level', 'general')
            pii_entities = item.get('pii_entities', [])
            
            start_time = time.time()
            
            try:
                # 1. PIIæ£€æµ‹
                pii_detected = self._detect_pii(text)
                
                # 2. éšç§ä¿æŠ¤
                protected_text = self._protect_privacy(text, pii_detected)
                
                # 3. è¾¹ç¼˜å¤„ç†
                edge_result = self._process_edge(protected_text)
                
                # 4. äº‘ç«¯å¤„ç†
                cloud_result = self._process_cloud(edge_result)
                
                # 5. ç»“æœè¿”å›
                final_result = self._return_result(cloud_result)
                
                end_time = time.time()
                
                workflow_results.append({
                    'input_text': text,
                    'input_label': label,
                    'privacy_level': privacy_level,
                    'pii_entities': pii_entities,
                    'pii_detected': pii_detected,
                    'processing_time': end_time - start_time,
                    'success': True,
                    'result': final_result
                })
                
            except Exception as e:
                workflow_results.append({
                    'input_text': text,
                    'input_label': label,
                    'privacy_level': privacy_level,
                    'pii_entities': pii_entities,
                    'error': str(e),
                    'success': False
                })
        
        # è®¡ç®—æˆåŠŸç‡
        successful_cases = sum(1 for r in workflow_results if r['success'])
        success_rate = successful_cases / len(workflow_results)
        
        workflow_metrics = {
            'dataset_name': dataset_name,
            'sample_count': len(data),
            'successful_cases': successful_cases,
            'success_rate': success_rate,
            'avg_processing_time': np.mean([r.get('processing_time', 0) for r in workflow_results if r['success']])
        }
        
        self.workflow_metrics[dataset_name] = workflow_metrics
        
        print_success(f"ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ:")
        print(f"   æˆåŠŸæ¡ˆä¾‹: {successful_cases}/{len(workflow_results)}")
        print(f"   æˆåŠŸç‡: {success_rate:.4f}")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {workflow_metrics['avg_processing_time']:.4f}ç§’")
        
        return workflow_metrics
    
    def _detect_pii(self, text: str) -> List[Dict[str, Any]]:
        """æ¨¡æ‹ŸPIIæ£€æµ‹"""
        pii_patterns = {
            'phone': r'\d{11}',
            'email': r'\w+@\w+\.\w+',
            'id_card': r'\d{18}',
            'name': r'[å¼ ç‹æèµµåˆ˜é™ˆæ¨é»„å´å‘¨å¾å­™é©¬æœ±èƒ¡éƒ­ä½•é«˜æ—ç½—éƒ‘æ¢è°¢å®‹å”è®¸éŸ©å†¯é‚“æ›¹å½­æ›¾è§ç”°è‘£è¢æ½˜äºè’‹è”¡ä½™æœå¶ç¨‹è‹é­å•ä¸ä»»æ²ˆå§šå¢å§œå´”é’Ÿè°­é™†æ±ªèŒƒé‡‘çŸ³å»–è´¾å¤éŸ¦ä»˜æ–¹ç™½é‚¹å­Ÿç†Šç§¦é‚±æ±Ÿå°¹è–›é—«æ®µé›·ä¾¯é¾™å²é™¶é»è´ºé¡¾æ¯›éƒé¾šé‚µä¸‡é’±ä¸¥è¦ƒæ­¦æˆ´è«å­”å‘æ±¤][\u4e00-\u9fa5]{1,2}'
        }
        
        detected_pii = []
        for pii_type, pattern in pii_patterns.items():
            import re
            matches = re.findall(pattern, text)
            for match in matches:
                detected_pii.append({
                    'type': pii_type,
                    'text': match,
                    'start': text.find(match),
                    'end': text.find(match) + len(match)
                })
        
        return detected_pii
    
    def _protect_privacy(self, text: str, pii_detected: List[Dict[str, Any]]) -> str:
        """æ¨¡æ‹Ÿéšç§ä¿æŠ¤"""
        protected_text = text
        for pii in pii_detected:
            protected_text = protected_text.replace(pii['text'], '*' * len(pii['text']))
        return protected_text
    
    def _process_edge(self, text: str) -> str:
        """æ¨¡æ‹Ÿè¾¹ç¼˜å¤„ç†"""
        return f"è¾¹ç¼˜å¤„ç†: {text}"
    
    def _process_cloud(self, text: str) -> str:
        """æ¨¡æ‹Ÿäº‘ç«¯å¤„ç†"""
        return f"äº‘ç«¯å¤„ç†: {text}"
    
    def _return_result(self, text: str) -> str:
        """æ¨¡æ‹Ÿç»“æœè¿”å›"""
        return f"æœ€ç»ˆç»“æœ: {text}"
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print_step("ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = sum(metrics['sample_count'] for metrics in self.performance_metrics.values())
        total_time = sum(metrics['processing_time'] for metrics in self.performance_metrics.values())
        avg_throughput = sum(metrics['throughput'] for metrics in self.performance_metrics.values()) / len(self.performance_metrics)
        
        total_pii = sum(metrics['total_pii_detected'] for metrics in self.privacy_metrics.values())
        avg_privacy_score = np.mean([metrics['avg_privacy_score'] for metrics in self.privacy_metrics.values()])
        avg_protection_rate = np.mean([metrics['privacy_protection_rate'] for metrics in self.privacy_metrics.values()])
        
        total_successful = sum(metrics['successful_cases'] for metrics in self.workflow_metrics.values())
        total_cases = sum(metrics['sample_count'] for metrics in self.workflow_metrics.values())
        overall_success_rate = total_successful / total_cases if total_cases > 0 else 0
        avg_processing_time = np.mean([metrics['avg_processing_time'] for metrics in self.workflow_metrics.values()])
        
        comprehensive_report = {
            'test_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_environment': 'Google Colab',
            'test_status': 'success',
            'dataset_count': len(self.performance_metrics),
            'total_samples': total_samples,
            'performance_metrics': {
                'total_processing_time': total_time,
                'avg_throughput': avg_throughput,
                'avg_processing_time': total_time / total_samples if total_samples > 0 else 0,
                'memory_usage': psutil.virtual_memory().percent,
                'gpu_available': torch.cuda.is_available()
            },
            'privacy_protection_metrics': {
                'total_pii_detected': total_pii,
                'avg_privacy_score': avg_privacy_score,
                'avg_protection_rate': avg_protection_rate,
                'privacy_compliance': 'PIPL compliant'
            },
            'end_to_end_metrics': {
                'total_successful_cases': total_successful,
                'total_cases': total_cases,
                'overall_success_rate': overall_success_rate,
                'avg_processing_time': avg_processing_time
            },
            'detailed_results': {
                'performance': self.performance_metrics,
                'privacy': self.privacy_metrics,
                'workflow': self.workflow_metrics
            },
            'overall_score': {
                'performance_score': min(1.0, avg_throughput / 10),
                'privacy_score': avg_protection_rate,
                'reliability_score': overall_success_rate,
                'overall_score': (min(1.0, avg_throughput / 10) + avg_protection_rate + overall_success_rate) / 3
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open('colab_chnsenticorp_test_report.json', 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        print_success("ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: colab_chnsenticorp_test_report.json")
        
        return comprehensive_report
    
    def generate_visualization(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print_step("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. æ€§èƒ½å¯¹æ¯”
        datasets = list(self.performance_metrics.keys())
        throughputs = [self.performance_metrics[ds]['throughput'] for ds in datasets]
        processing_times = [self.performance_metrics[ds]['avg_time_per_sample'] for ds in datasets]
        
        axes[0, 0].bar(datasets, throughputs, color=['blue', 'green', 'red', 'orange', 'purple'][:len(datasets)])
        axes[0, 0].set_title('ååé‡å¯¹æ¯”')
        axes[0, 0].set_ylabel('æ ·æœ¬/ç§’')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        axes[0, 1].bar(datasets, processing_times, color=['blue', 'green', 'red', 'orange', 'purple'][:len(datasets)])
        axes[0, 1].set_title('å¹³å‡å¤„ç†æ—¶é—´å¯¹æ¯”')
        axes[0, 1].set_ylabel('ç§’/æ ·æœ¬')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 2. éšç§ä¿æŠ¤å¯¹æ¯”
        privacy_scores = [self.privacy_metrics[ds]['avg_privacy_score'] for ds in datasets]
        protection_rates = [self.privacy_metrics[ds]['privacy_protection_rate'] for ds in datasets]
        
        axes[1, 0].bar(datasets, privacy_scores, color=['blue', 'green', 'red', 'orange', 'purple'][:len(datasets)])
        axes[1, 0].set_title('å¹³å‡éšç§åˆ†æ•°å¯¹æ¯”')
        axes[1, 0].set_ylabel('éšç§åˆ†æ•°')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        axes[1, 1].bar(datasets, protection_rates, color=['blue', 'green', 'red', 'orange', 'purple'][:len(datasets)])
        axes[1, 1].set_title('éšç§ä¿æŠ¤ç‡å¯¹æ¯”')
        axes[1, 1].set_ylabel('ä¿æŠ¤ç‡')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('colab_chnsenticorp_test_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print_success("å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: colab_chnsenticorp_test_results.png")

def run_chnsenticorp_dataset_test():
    """è¿è¡ŒChnSentiCorpæ•°æ®é›†æµ‹è¯•"""
    print_header("Colabç¯å¢ƒä¸‹çš„ChnSentiCorpæ•°æ®é›†å¯¼å…¥å’Œæµ‹è¯•")
    
    # 1. åˆå§‹åŒ–æ•°æ®é›†å¯¼å…¥å™¨
    importer = ChnSentiCorpDatasetImporter()
    
    # 2. ä¸Šä¼ æœ¬åœ°æ•°æ®é›†åˆ°Colab
    print_step("ä¸Šä¼ ChnSentiCorpæ•°æ®é›†åˆ°Colab")
    
    # æœ¬åœ°æ•°æ®é›†è·¯å¾„
    local_data_path = r"D:\ianvs\examples\OSPP-implemention-Proposal-PIPL-Compliant-Cloud-Edge-Collaborative-Privacy-Preserving-Prompt-Processing-Framework-203\edge-cloud_collaborative_learning_bench\data\chnsenticorp_lite"
    
    # ä¸Šä¼ æ•°æ®é›†
    success = importer.upload_local_dataset(local_data_path, "chnsenticorp")
    
    if not success:
        print_error("ChnSentiCorpæ•°æ®é›†ä¸Šä¼ å¤±è´¥")
        return False
    
    # 3. å¯¼å…¥æ•°æ®é›†
    print_step("å¯¼å…¥ChnSentiCorpæ•°æ®é›†")
    colab_data_path = os.path.join(importer.base_path, "chnsenticorp")
    import_success = importer.import_chnsenticorp_dataset(colab_data_path)
    
    if not import_success:
        print_error("ChnSentiCorpæ•°æ®é›†å¯¼å…¥å¤±è´¥")
        return False
    
    # 4. æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    print_step("æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯")
    datasets = importer.list_datasets()
    print(f"å·²å¯¼å…¥çš„æ•°æ®é›†: {datasets}")
    
    for dataset_name in datasets:
        dataset_info = importer.get_dataset_info(dataset_name)
        print(f"\næ•°æ®é›†åç§°: {dataset_info['name']}")
        print(f"æ•°æ®é›†å¤§å°: {dataset_info['size']}")
        print(f"æ•°æ®æ ¼å¼: {dataset_info['format']}")
        print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {dataset_info['text_stats']['avg_length']:.2f}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {dataset_info['label_distribution']}")
        print(f"éšç§çº§åˆ«åˆ†å¸ƒ: {dataset_info['privacy_level_distribution']}")
        print(f"PIIç»Ÿè®¡: {dataset_info['pii_stats']}")
    
    # 5. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if importer.statistics:
        print_step("æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
        stats = importer.get_statistics()
        print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"è®­ç»ƒé›†: {stats['train_samples']}")
        print(f"éªŒè¯é›†: {stats['val_samples']}")
        print(f"æµ‹è¯•é›†: {stats['test_samples']}")
        print(f"éšç§çº§åˆ«åˆ†å¸ƒ: {stats['privacy_levels']}")
        print(f"PIIå®ä½“åˆ†å¸ƒ: {stats['pii_entities']}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ: {stats['labels']}")
        print(f"è·¨å¢ƒä¼ è¾“åˆ†å¸ƒ: {stats['cross_border']}")
    
    # 6. åˆ›å»ºæµ‹è¯•å™¨
    test_config = {
        'edge_model': {
            'name': 'colab_unsloth_model',
            'colab_model_name': 'Qwen/Qwen2.5-7B-Instruct',
            'quantization': '4bit',
            'max_length': 2048,
            'use_lora': True,
            'unsloth_optimized': True
        },
        'cloud_model': {
            'name': 'colab_unsloth_model',
            'colab_model_name': 'Qwen/Qwen2.5-7B-Instruct',
            'quantization': '4bit',
            'max_tokens': 1024,
            'use_lora': True,
            'unsloth_optimized': True
        },
        'privacy_detection': {
            'detection_methods': {
                'regex_patterns': ['phone', 'id_card', 'email', 'address', 'name']
            }
        },
        'privacy_encryption': {
            'differential_privacy': {
                'general': {
                    'epsilon': 1.2,
                    'delta': 0.00001,
                    'clipping_norm': 1.0
                }
            }
        },
        'compliance_monitoring': {
            'pipl_compliance': True,
            'cross_border_validation': True,
            'audit_logging': True
        }
    }
    
    tester = ChnSentiCorpDatasetTester(test_config)
    print_success("ChnSentiCorpæ•°æ®é›†æµ‹è¯•å™¨åˆ›å»ºå®Œæˆ")
    
    # 7. è¿è¡Œæµ‹è¯•
    print_step("å¼€å§‹ChnSentiCorpæ•°æ®é›†æµ‹è¯•")
    
    # æµ‹è¯•è®­ç»ƒé›†
    if 'train' in datasets:
        print("\n" + "="*50)
        print("ğŸ“Š è®­ç»ƒé›†æµ‹è¯•")
        print("="*50)
        train_data = importer.get_dataset('train')
        tester.test_dataset_performance("train", train_data)
        tester.test_dataset_privacy("train", train_data)
        tester.test_dataset_workflow("train", train_data)
    
    # æµ‹è¯•éªŒè¯é›†
    if 'val' in datasets:
        print("\n" + "="*50)
        print("ğŸ“Š éªŒè¯é›†æµ‹è¯•")
        print("="*50)
        val_data = importer.get_dataset('val')
        tester.test_dataset_performance("val", val_data)
        tester.test_dataset_privacy("val", val_data)
        tester.test_dataset_workflow("val", val_data)
    
    # æµ‹è¯•æµ‹è¯•é›†
    if 'test' in datasets:
        print("\n" + "="*50)
        print("ğŸ“Š æµ‹è¯•é›†æµ‹è¯•")
        print("="*50)
        test_data = importer.get_dataset('test')
        tester.test_dataset_performance("test", test_data)
        tester.test_dataset_privacy("test", test_data)
        tester.test_dataset_workflow("test", test_data)
    
    # 8. ç”ŸæˆæŠ¥å‘Š
    print_step("ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
    report = tester.generate_comprehensive_report()
    
    # 9. ç”Ÿæˆå¯è§†åŒ–
    tester.generate_visualization()
    
    # 10. æœ€ç»ˆæ€»ç»“
    print_header("æµ‹è¯•å®Œæˆ")
    print_success("ChnSentiCorpæ•°æ®é›†æµ‹è¯•å®Œæˆï¼")
    print_success("æ•°æ®é›†ä¸Šä¼ å®Œæˆ")
    print_success("æ•°æ®é›†å¯¼å…¥å®Œæˆ")
    print_success("æ•°æ®é›†éªŒè¯å®Œæˆ")
    print_success("åˆ†æ­¥æµ‹è¯•è¿è¡Œå®Œæˆ")
    print_success("ç»“æœåˆ†æå®Œæˆ")
    print_success("å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    print("\nğŸ“‹ æµ‹è¯•ç»“æœ:")
    print(f"   æ€»æ ·æœ¬æ•°: {report['total_samples']}")
    print(f"   æ€»å¤„ç†æ—¶é—´: {report['performance_metrics']['total_processing_time']:.2f}ç§’")
    print(f"   å¹³å‡ååé‡: {report['performance_metrics']['avg_throughput']:.2f}æ ·æœ¬/ç§’")
    print(f"   æ€»PIIæ£€æµ‹æ•°: {report['privacy_protection_metrics']['total_pii_detected']}")
    print(f"   å¹³å‡ä¿æŠ¤ç‡: {report['privacy_protection_metrics']['avg_protection_rate']:.4f}")
    print(f"   æ€»ä½“æˆåŠŸç‡: {report['end_to_end_metrics']['overall_success_rate']:.4f}")
    
    print("\nğŸ¯ å…³é”®æˆå°±:")
    print("- âœ… ChnSentiCorpæ•°æ®é›†ä¸Šä¼ æˆåŠŸ")
    print("- âœ… æ•°æ®é›†å¯¼å…¥å®Œæˆ")
    print("- âœ… æ•°æ®é›†éªŒè¯å®Œæˆ")
    print("- âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
    print("- âœ… éšç§ä¿æŠ¤æµ‹è¯•å®Œæˆ")
    print("- âœ… ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•å®Œæˆ")
    print("- âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    print("\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
    print("- colab_chnsenticorp_test_report.json (è¯¦ç»†æŠ¥å‘Š)")
    print("- colab_chnsenticorp_test_results.png (å¯è§†åŒ–å›¾è¡¨)")
    
    print("\nğŸš€ æµ‹è¯•å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°Colabç¯å¢ƒä¸­ã€‚")
    
    return True

if __name__ == "__main__":
    success = run_chnsenticorp_dataset_test()
    if success:
        print("\nğŸ‰ æµ‹è¯•æˆåŠŸï¼ChnSentiCorpæ•°æ®é›†æµ‹è¯•è¿è¡Œæ­£å¸¸")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–")
        sys.exit(1)
