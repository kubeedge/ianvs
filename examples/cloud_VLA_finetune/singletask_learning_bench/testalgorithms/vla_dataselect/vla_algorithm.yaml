<<<<<<< HEAD
algorithm:
  # paradigm name; string type;
  # currently the options of value are as follows:
  #   1> "singletasklearning"
  #   2> "incrementallearning"
  paradigm_type: "singletasklearning"
  # the url address of initial model; string type; optional;
  initial_model_url: "/inspire/hdd/global_user/chaimingxu-240108540141/jwq-test/model/finetune_model/models--openvla--openvla-7b+libero_10_no_noops+b8+lr-0.0005+lora-r32+dropout-0.0--image_aug/model.zip"
  # algorithm module configuration in the paradigm; list type;
  modules:
    # kind of algorithm module; string type;
    # currently the options of value are as follows:
    #   1> "basemodel"
    - type: "basemodel"
      # name of python module; string type;
      # example: basemodel.py has BaseModel module that the alias is "FPN" for this benchmarking;
      name: "OpenVLA_Finetune"
      # the url address of python module; string type;
      url: "/inspire/hdd/global_user/chaimingxu-240108540141/jwq-test/ianvs/examples/cloud_VLA_finetune/singletask_learning_bench/testalgorithms/vla_dataselect/basemodel.py"

      # # hyperparameters configuration for the python module; list type;
      hyperparameters:
        # name of the hyperparameter; string type;
            # values of the hyperparameter; list type;
            # types of the value are string/int/float/boolean/list/dictionary
        - dataset_name:
            values:
              - "libero_object_no_noops" # dataset name: such as libero_10_no_noops
        - task_suite_name:
            values:
              - "libero_object" # Task suite. Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90
        - num_trials_per_task:
            values:
              - 2

              
=======
version https://git-lfs.github.com/spec/v1
oid sha256:0900a05f9326dd4653f59ad1315d4c73e2f9bf4d99a128ea2258bf9af626e68f
size 1740
>>>>>>> 9676c3e (ya toh aar ya toh par)
